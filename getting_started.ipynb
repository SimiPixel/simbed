{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started with the *Simulation Testbed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just one environment for now, let's load it\n",
    "Note: A TWIPR would be a nice env for future work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 11:29:48.288234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-14 11:29:48.288255: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  if hasattr(pil_image, 'HAMMING'):\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  if hasattr(pil_image, 'BOX'):\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  if hasattr(pil_image, 'LANCZOS'):\n",
      "/home/simon/miniforge3/envs/simbed/lib/python3.9/site-packages/reverb/platform/default/ensure_tf_install.py:53: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(version) <\n",
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from simbed.envs.two_segments import load_segment_env\n",
    "from simbed.common.utils import generate_ts\n",
    "\n",
    "time_limit=5\n",
    "control_timestep=0.01\n",
    "\n",
    "env = load_segment_env(time_limit=time_limit, control_timestep=control_timestep)\n",
    "ts = generate_ts(time_limit, control_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Therefore the length of an episode trajectory will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "501"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = len(ts)+1 # +1 for the initial state (no action performed yet)\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And we apply inputs at those timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0.        , 0.01      , 0.02      , 0.03      , 0.04      ,\n",
       "             0.05      , 0.06      , 0.07      , 0.08      , 0.09      ,\n",
       "             0.09999999, 0.11      , 0.12      , 0.13      , 0.14      ,\n",
       "             0.14999999, 0.16      , 0.17      , 0.17999999, 0.19      ],            dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The input to the system and observation from the system is of the form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundedArray(shape=(1,), dtype=dtype('float32'), name=None, minimum=[-1.e+10], maximum=[1.e+10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeStep(step_type=<StepType.FIRST: 0>, reward=None, discount=None, observation=OrderedDict([('xpos_of_segment_end', array([2.5717583e-16], dtype=float32))]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step([0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice how there is no reward, how could there be? We have to add an observation reference trajectory first to quantify some error and return it as reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now create an arbitrary smooth input-trajectory, record the observation of that input as reference and store it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmp2oqx_16b.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:386] Loading latest checkpoint from /tmp/tmp2oqx_16b\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 37895\n",
      "2022-07-14 11:29:52.220853: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-07-14 11:29:52.220878: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1589735) so Table ref_buffer is accessed directly without gRPC.\n",
      "[reverb/cc/platform/default/server.cc:84] Shutting down replay server\n"
     ]
    }
   ],
   "source": [
    "from simbed.common.observation_ref_source import make_obs_ref_source\n",
    "# in fact we sample 3 smooth input-trajectories\n",
    "observation_reference_source = make_obs_ref_source(env, ts, seeds=[4,5,6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the the seed `4` trajectory as observation reference for the actor \n",
    "That doesn't not yet imply that the actor will use it, a RandomlyActing-Actor e.g. does not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=0\n",
    "observation_reference_source.change_reference_of_actor(i) # because\n",
    "[4,5,6][i] == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's look at that observation reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2.5717583e-16],\n",
       "             [ 2.5717583e-16],\n",
       "             [ 2.4670944e-08],\n",
       "             [ 7.7913448e-08],\n",
       "             [ 8.4103661e-08],\n",
       "             [-1.1150199e-07],\n",
       "             [-7.2781575e-07],\n",
       "             [-2.0222394e-06],\n",
       "             [-4.2590200e-06],\n",
       "             [-7.6765864e-06]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the observation is in general a dictionary\n",
    "# here the dictionary has only one entry `xpos_of_segmend_end`\n",
    "observation_reference_source.get_reference_actor()[\"xpos_of_segment_end\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What input-signal caused that observation reference, aka what is the input-signal we want to find (and we don't know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.        ],\n",
       "             [0.00049556],\n",
       "             [0.00094046],\n",
       "             [0.00133508],\n",
       "             [0.00167975],\n",
       "             [0.00197491],\n",
       "             [0.00222093],\n",
       "             [0.00241827],\n",
       "             [0.00256738],\n",
       "             [0.00266873]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_reference_source._uss[observation_reference_source._i_actor, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's also add that reference to the environment such that there can be a reward signal (negative MSE between actual and reference observation)\n",
    "This is mostly for completeness i wouldn't really use the reward probably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = load_segment_env(observation_reference_source, time_limit, control_timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-1.2677453e-06, dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    timestep = env.step([0.1])\n",
    "timestep.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can even change the observation reference on the fly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "observation_reference_source.change_reference_of_actor(1) # seed 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(-2.7006658e-06, dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    timestep = env.step([0.1])\n",
    "timestep.reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import viewer\n",
    "\n",
    "# press spacebar to start\n",
    "# press delete to reset\n",
    "\n",
    "viewer.launch(load_segment_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Not much going on .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a random actor and look at it interacting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simbed.actors.random_actor import RandomlyActingActor\n",
    "\n",
    "actor = RandomlyActingActor(action_spec=env.action_spec(), ts=ts, reset_key=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acme.environment_loop import EnvironmentLoop\n",
    "\n",
    "loop = EnvironmentLoop(env, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_length': 500,\n",
       " 'episode_return': array(-29254.098, dtype=float32),\n",
       " 'steps_per_second': 397.41586603375305,\n",
       " 'episodes': 1,\n",
       " 'steps': 500}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorStateless(key=DeviceArray([117693863, 924251068], dtype=uint32), count=500)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the actor has a state: a random seed, and a counter\n",
    "actor._state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import viewer\n",
    "from simbed.common.utils import actor2launch_policy\n",
    "\n",
    "viewer.launch(load_segment_env, actor2launch_policy(actor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at a FeedForward-Actor and run the input-signal that causes our observation-reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simbed.actors.feedforward_actor import FeedforwardActor\n",
    "\n",
    "u_ref = observation_reference_source._uss[observation_reference_source._i_actor]\n",
    "actor = FeedforwardActor(u_ref, action_spec=env.action_spec(), ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dm_control import viewer\n",
    "from simbed.common.utils import actor2launch_policy\n",
    "\n",
    "viewer.launch(load_segment_env, policy=actor2launch_policy(actor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's store the data of some trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmp2a54prmj.\n",
      "[reverb/cc/platform/tfrecord_checkpointer.cc:386] Loading latest checkpoint from /tmp/tmp2a54prmj\n",
      "[reverb/cc/platform/default/server.cc:71] Started replay server on port 36337\n"
     ]
    }
   ],
   "source": [
    "from simbed.common.buffer import make_adder, make_data_storage, DataIteratorFromDataStorage\n",
    "\n",
    "# This is the object that stores all transition steps / timestep objects\n",
    "data_storage = make_data_storage(env, \"replay-buffer\", ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Client, server_address=thinkpad-x1:36337"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_storage.table # this specifies how the storage is structured\n",
    "data_storage.client # this specifies how to interface with the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the object that tells the actor how to get data into the data storage\n",
    "adder = make_adder(data_storage.client, \"replay-buffer\", ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[reverb/cc/client.cc:165] Sampler and server are owned by the same process (1589735) so Table replay-buffer is accessed directly without gRPC.\n"
     ]
    }
   ],
   "source": [
    "# This is an iterator object that allows us to iterate over the episodes in the data-storage\n",
    "iterator = DataIteratorFromDataStorage(data_storage.client, data_storage.table)\n",
    "sample = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample # right now its empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simbed.actors.random_actor import RandomlyActingActor\n",
    "\n",
    "actor = RandomlyActingActor(action_spec=env.action_spec(), ts=ts, reset_key=True, adder=adder)\n",
    "\n",
    "from acme.environment_loop import EnvironmentLoop\n",
    "\n",
    "loop = EnvironmentLoop(env, actor)\n",
    "loop.run_episode()\n",
    "sample = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 2.5717583e-16],\n",
       "             [-7.8200756e-05],\n",
       "             [-1.7674586e-04],\n",
       "             [-4.5510009e-05],\n",
       "             [ 7.4616936e-04]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.observation[\"xpos_of_segment_end\"][0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-7.4862648e+09],\n",
       "             [-7.2523904e+09],\n",
       "             [-2.5750541e+09],\n",
       "             [ 4.8829056e+09],\n",
       "             [-7.3938084e+09]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.action[0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 2.5717583e-16],\n",
       "              [-7.8200756e-05],\n",
       "              [-1.7674586e-04],\n",
       "              [-4.5510009e-05],\n",
       "              [ 7.4616936e-04]],\n",
       "\n",
       "             [[ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00]],\n",
       "\n",
       "             [[ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.observation[\"xpos_of_segment_end\"][0:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop.run_episode()\n",
    "sample = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[ 2.5717583e-16],\n",
       "              [-7.8200756e-05],\n",
       "              [-1.7674586e-04],\n",
       "              [-4.5510009e-05],\n",
       "              [ 7.4616936e-04]],\n",
       "\n",
       "             [[ 2.5717583e-16],\n",
       "              [-7.8200756e-05],\n",
       "              [-1.7674586e-04],\n",
       "              [-4.5510009e-05],\n",
       "              [ 7.4616936e-04]],\n",
       "\n",
       "             [[ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.observation[\"xpos_of_segment_end\"][0:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[[-7.4862648e+09],\n",
       "              [-7.2523904e+09],\n",
       "              [-2.5750541e+09],\n",
       "              [ 4.8829056e+09],\n",
       "              [-7.3938084e+09]],\n",
       "\n",
       "             [[-7.4862648e+09],\n",
       "              [-7.2523904e+09],\n",
       "              [-2.5750541e+09],\n",
       "              [ 4.8829056e+09],\n",
       "              [-7.3938084e+09]],\n",
       "\n",
       "             [[ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00],\n",
       "              [ 0.0000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.action[0:3,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What about the rewards?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-6.1153580e-09, -3.1176576e-08, -2.0191617e-09,\n",
       "              -5.5777127e-07, -4.7538033e-06],\n",
       "             [-6.1153580e-09, -3.1176576e-08, -2.0191617e-09,\n",
       "              -5.5777127e-07, -4.7538033e-06],\n",
       "             [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "               0.0000000e+00,  0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.reward[0:3,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_reference_source.change_reference_of_actor(2)\n",
    "loop.run_episode()\n",
    "sample = next(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-6.1153580e-09, -3.1176576e-08, -2.0191617e-09,\n",
       "              -5.5777127e-07, -4.7538033e-06],\n",
       "             [-6.1153580e-09, -3.1176576e-08, -2.0191617e-09,\n",
       "              -5.5777127e-07, -4.7538033e-06],\n",
       "             [-6.1153580e-09, -3.1232798e-08, -2.0655206e-09,\n",
       "              -5.5689583e-07, -4.7565804e-06]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.data.reward[0:3,:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Notice how the last episode reward is different even though it's the same randomlyActing-Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a NeuralNetwork-Actor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The NeuralNetwork-Actor consists of a simple MLP-Policy. The input of the Policy is the current observation and the current reference observation (as it is specificed by the `observation_reference_source`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from simbed.common.types import nn \n",
    "from simbed.common.utils import idx_in_pytree\n",
    "\n",
    "class PolicyInputCurrentReferenceOnly(NamedTuple):\n",
    "    observation: nn.Observation\n",
    "    observation_ref: nn.Observation\n",
    "\n",
    "def preprocess_current_reference_only(\n",
    "        obs: nn.Observation, \n",
    "        obss_ref: nn.Observations, \n",
    "        timestep: int\n",
    "    ):\n",
    "    return PolicyInputCurrentReferenceOnly(\n",
    "        obs, idx_in_pytree(obss_ref, start=timestep)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simbed.common.nn_lib.mlp_policy import MLPPolicy\n",
    "\n",
    "policy = MLPPolicy(\n",
    "    preprocess=preprocess_current_reference_only,\n",
    "    hidden_layers=[32,1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Parameters of the MLP-Policy are maintained not by the `actor` but by the `learner`. The `actor` calls the `leaner` for the most current set of parameters. \n",
    "\n",
    "The `leaner` uses the `iterator` over the `data-storage` to update the parameters. \n",
    "\n",
    "The `actor` interacts with the `environment` and uses the `adder` to fill the `data-storage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simbed.learners.learner import NoLearningLearner\n",
    "\n",
    "learner = NoLearningLearner(policy=policy, iterator=iterator, observation_ref_source=observation_reference_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simbed.actors.neural_network_actor import NeuralNetworkActor\n",
    "\n",
    "actor = NeuralNetworkActor(policy=policy, ref_obs_source=observation_reference_source, source=learner, action_spec=env.action_spec(), ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this changes not only the reward function\n",
    "observation_reference_source.change_reference_of_actor(0)\n",
    "# but also what the MLP-Policy sees as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = EnvironmentLoop(env, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_length': 500,\n",
       " 'episode_return': array(-148647.61, dtype=float32),\n",
       " 'steps_per_second': 260.9462891510827,\n",
       " 'episodes': 1,\n",
       " 'steps': 500}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop.run_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.110027  ,  0.2826411 , -0.5364699 ,  0.49340966,\n",
       "               0.29420245,  1.1756164 , -0.37984464,  0.07116422,\n",
       "               0.21966776,  1.1451848 , -0.13390402,  0.12129781,\n",
       "              -0.08688062, -0.19010076,  0.6919145 , -1.071506  ,\n",
       "               0.651455  ,  1.032536  ,  0.6642359 ,  0.03351547,\n",
       "              -0.83725995, -0.8653808 ,  1.1468529 ,  1.3384238 ,\n",
       "              -0.13897334, -0.65209335, -0.8341206 ,  0.1803869 ,\n",
       "              -0.6710531 ,  0.08734557, -0.32259268,  0.61759573],\n",
       "             [-0.06630728,  0.48761335, -0.66747767, -0.48171386,\n",
       "              -0.32224986, -0.11225194, -0.50119233,  0.5764158 ,\n",
       "              -0.42776665,  0.9026953 ,  0.5698295 , -0.05112315,\n",
       "               0.28485522,  0.35889995, -1.0724318 , -0.21618941,\n",
       "               0.19230606, -1.3536115 , -0.61899096,  0.07826599,\n",
       "              -0.6793999 ,  0.83316565, -0.5243171 , -0.48312548,\n",
       "               0.8055845 ,  0.25807765, -0.00221061, -0.8041787 ,\n",
       "               0.18043563, -0.42203048,  0.7996217 , -0.36087584]],            dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor._params['mlp/~/linear_0'][\"w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.step(n_gradient_steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 1.110027  ,  0.2826411 , -0.5364699 ,  0.49340966,\n",
       "               0.29420245,  1.1756164 , -0.37984464,  0.07116422,\n",
       "               0.21966776,  1.1451848 , -0.13390402,  0.12129781,\n",
       "              -0.08688062, -0.19010076,  0.6919145 , -1.071506  ,\n",
       "               0.651455  ,  1.032536  ,  0.6642359 ,  0.03351547,\n",
       "              -0.83725995, -0.8653808 ,  1.1468529 ,  1.3384238 ,\n",
       "              -0.13897334, -0.65209335, -0.8341206 ,  0.1803869 ,\n",
       "              -0.6710531 ,  0.08734557, -0.32259268,  0.61759573],\n",
       "             [-0.06630728,  0.48761335, -0.66747767, -0.48171386,\n",
       "              -0.32224986, -0.11225194, -0.50119233,  0.5764158 ,\n",
       "              -0.42776665,  0.9026953 ,  0.5698295 , -0.05112315,\n",
       "               0.28485522,  0.35889995, -1.0724318 , -0.21618941,\n",
       "               0.19230606, -1.3536115 , -0.61899096,  0.07826599,\n",
       "              -0.6793999 ,  0.83316565, -0.5243171 , -0.48312548,\n",
       "               0.8055845 ,  0.25807765, -0.00221061, -0.8041787 ,\n",
       "               0.18043563, -0.42203048,  0.7996217 , -0.36087584]],            dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor._params['mlp/~/linear_0'][\"w\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Well the `NoLearningLeaner` doesn't learn .. but yes that is it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('simbed')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "604dff45da25a60b58291261dc7a03791595928e73c3bd388c1b538d9524c8be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
